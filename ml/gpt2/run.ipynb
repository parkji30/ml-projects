{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "from torch import nn\n",
    "import os\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "input_file_path = os.path.join('input.txt')\n",
    "\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for c, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "tensor = torch.tensor(encode(text[:1000]), dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data length: 1003854, \n",
      "val data length: 111540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0.9 * len(text)\n",
    "train_data = encode(text)[:int(n)]\n",
    "val_data = encode(text)[int(n):]\n",
    "\n",
    "print(f\"train data length: {len(train_data)}, \\nval data length: {len(val_data)}\")\n",
    "\n",
    "seq_length = 8\n",
    "\n",
    "train_data[:seq_length + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8]) torch.Size([8, 8])\n",
      "tensor([ 1, 61, 47, 58, 46,  1, 46, 47]) tensor([61, 47, 58, 46,  1, 46, 47, 57])\n",
      "Context [1], Target 61\n",
      "Context [1, 61], Target 47\n",
      "Context [1, 61, 47], Target 58\n",
      "Context [1, 61, 47, 58], Target 46\n",
      "Context [1, 61, 47, 58, 46], Target 1\n",
      "Context [1, 61, 47, 58, 46, 1], Target 46\n",
      "Context [1, 61, 47, 58, 46, 1, 46], Target 47\n",
      "Context [1, 61, 47, 58, 46, 1, 46, 47], Target 57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seq length = 8\n",
    "batch_size = 8\n",
    "def get_batch(split, seq_length):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    ix = torch.randint(len(data) - seq_length, (batch_size, 1))\n",
    "    context_tensor = torch.stack([torch.tensor(data[i:i+seq_length]) for i in ix])\n",
    "    response_tensor = torch.stack([torch.tensor(data[i+1:i+seq_length+1]) for i in ix])\n",
    "    return context_tensor, response_tensor\n",
    "\n",
    "context, response = get_batch('train', 8)\n",
    "print(context.shape, response.shape)\n",
    "print(context[0], response[0])\n",
    "for b in range(batch_size):\n",
    "    for t in range(seq_length):\n",
    "        # Remember when you index, it doesn't include your stopping point\n",
    "        print(f\"Context {context[b][:t + 1].tolist()}, Target {response[b][t].tolist()}\")\n",
    "    break # just print first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, context, response=None):\n",
    "        # we are guarenteed idx = vocab size \n",
    "        # since idx is based on the stoi, itos dictionaries\n",
    "        logits = self.embedding(context)\n",
    "        if response is None:\n",
    "            return logits\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.reshape(B*T, C)\n",
    "            response = response.reshape(B*T,)\n",
    "            loss = F.cross_entropy(logits, response)\n",
    "            return logits, loss \n",
    "\n",
    "    def generate(self, context, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(context)\n",
    "            # get only the last timestep\n",
    "            logits = logits[:, -1, :]\n",
    "            # convert your 65 dimensional embedding vector into\n",
    "            # a probabilty distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Next token prediction\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.concat((context, next_token), dim=1)\n",
    "        return context\n",
    "    \n",
    "bigram = BigramLanguageModel(len(stoi))\n",
    "# logits, loss = bigram(context, response)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gb::LBQYq?CJesxt\n",
      "yL3\n",
      "jv,\n",
      "'\n",
      "-AlOmW LNrxKbrnxgjhF$Lmek'nw-zeLZSRwPPKZ;iqq 3-&tEQ'SPiR L;fAgN'StSQUFq'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigram.generate(context, max_new_tokens=100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.3460681438446045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.582165479660034\n",
      "loss 2.3618199825286865\n",
      "loss 2.372929573059082\n",
      "loss 2.5345356464385986\n",
      "loss 2.385035753250122\n",
      "loss 2.312197208404541\n",
      "loss 2.685347080230713\n",
      "loss 2.241530656814575\n",
      "loss 2.4689106941223145\n",
      "loss 2.588299512863159\n",
      "loss 2.5005948543548584\n",
      "loss 2.6923959255218506\n",
      "loss 2.3396263122558594\n",
      "loss 2.3891985416412354\n",
      "loss 2.3035621643066406\n",
      "loss 2.4782638549804688\n",
      "loss 2.5053746700286865\n",
      "loss 2.5575125217437744\n",
      "loss 2.4796996116638184\n"
     ]
    }
   ],
   "source": [
    "steps = 10000\n",
    "optim = torch.optim.AdamW(bigram.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(steps):\n",
    "    context, response = get_batch('train', 8)\n",
    "\n",
    "    # Forward \n",
    "    logits, loss = bigram(context, response)\n",
    "    \n",
    "    # Zero the gradient out\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    # get gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update Parameters\n",
    "    optim.step()\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(f\"loss {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "D ond so worowes?\n",
      "To tes brbjLayouchodive melis isorome 'le all hifowen st hath berico athou RKI nou\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(bigram.generate(context, max_new_tokens=100).tolist()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
