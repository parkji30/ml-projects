{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow_og = torch.zeros((B, T, C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]\n",
    "        xbow_og[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.tril(torch.ones(T, T))\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "\n",
    "xbow = xbow @ x / wei.sum(1, keepdim=True)\n",
    "torch.allclose(xbow, xbow_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(wei == 0, float(\"-inf\"))\n",
    "print(wei.shape)\n",
    "F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"\"\"\n",
    "    Manual implementation of batch normalization for 1D case\n",
    "    x: input tensor \n",
    "    - For 2D input (N, C): normalize across batch dimension N\n",
    "    - For 3D input (N, C, L): normalize across batch N and length L dimensions\n",
    "    gamma: learnable scale parameter\n",
    "    beta: learnable shift parameter  \n",
    "    eps: small constant for numerical stability\n",
    "    \"\"\"\n",
    "\n",
    "stoi = {s: i for i, s in enumerate(set(list(str(text))))}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def manual_batch_norm_1d(x, gamma=None, beta=None, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Manual implementation of batch normalization for 1D case\n",
    "    x: input tensor \n",
    "    - For 2D input (N, C): normalize across batch dimension N\n",
    "    - For 3D input (N, C, L): normalize across batch N and length L dimensions\n",
    "    gamma: learnable scale parameter\n",
    "    beta: learnable shift parameter  \n",
    "    eps: small constant for numerical stability\n",
    "    \"\"\"\n",
    "    \n",
    "    if x.dim() == 2:\n",
    "        # For 2D input (N, C), normalize across batch dimension (dim=0)\n",
    "        mean = torch.mean(x, dim=0, keepdim=True)\n",
    "        var = torch.var(x, dim=0, keepdim=True, unbiased=False)\n",
    "    elif x.dim() == 3:\n",
    "        # For 3D input (N, C, L), normalize across batch and length dimensions (dim=(0,2))\n",
    "        mean = torch.mean(x, dim=(0, 2), keepdim=True)\n",
    "        var = torch.var(x, dim=(0, 2), keepdim=True, unbiased=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 2D or 3D input, got {x.dim()}D\")\n",
    "    \n",
    "    # Normalize: (x - mean) / sqrt(var + eps)\n",
    "    x_normalized = (x - mean) / torch.sqrt(var + eps)\n",
    "    \n",
    "    # Scale and shift (if parameters provided)\n",
    "    if gamma is not None:\n",
    "        x_normalized = x_normalized * gamma\n",
    "    if beta is not None:\n",
    "        x_normalized = x_normalized + beta\n",
    "        \n",
    "    return x_normalized, mean, var\n",
    "\n",
    "print(\"=== Problem with original implementation ===\")\n",
    "# Your original approach - WRONG for 3D input\n",
    "x = torch.tensor([[[1, 2],\n",
    "                           [3, 4]],\n",
    "\n",
    "                          [[5, 6],\n",
    "                           [7, 8]]], dtype=torch.float32)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input tensor:\\n{x}\")\n",
    "\n",
    "# # Wrong way (your original) - only normalizes across batch dim\n",
    "mean = torch.mean(x, dim=[0, 2], keepdim=True)\n",
    "print(mean)\n",
    "\n",
    "var = torch.var(x, dim=[0, 2], keepdim=True, unbiased=False)\n",
    "x_wrong = (x - mean) / torch.sqrt(var + 1e-5)\n",
    "print(x_wrong)\n",
    "bn = torch.nn.BatchNorm1d(2, track_running_stats=False)\n",
    "bn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Batch Norm Test Example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def test_batch_norm():\n",
    "    \"\"\"Test batch normalization with a simple example\"\"\"\n",
    "    \n",
    "    # Create test data: (batch_size, features)\n",
    "    torch.manual_seed(42)\n",
    "    batch_size, num_features = 8, 4\n",
    "    x = torch.randn(batch_size, num_features) * 3 + 2  # Random data with mean~2, std~3\n",
    "    \n",
    "    print(\"Original data:\")\n",
    "    print(f\"Shape: {x.shape}\")\n",
    "    print(f\"Mean per feature: {x.mean(dim=0)}\")\n",
    "    print(f\"Std per feature: {x.std(dim=0, unbiased=False)}\")\n",
    "    print()\n",
    "    \n",
    "    # Manual batch norm\n",
    "    def manual_batch_norm(x, eps=1e-5):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        var = x.var(dim=0, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "        return x_norm, mean, var\n",
    "    \n",
    "    # Apply manual batch norm\n",
    "    x_manual, mean, var = manual_batch_norm(x)\n",
    "    \n",
    "    print(\"After manual batch norm:\")\n",
    "    print(f\"Mean per feature: {x_manual.mean(dim=0)}\")\n",
    "    print(f\"Std per feature: {x_manual.std(dim=0, unbiased=False)}\")\n",
    "    print()\n",
    "    \n",
    "    # PyTorch's BatchNorm1d\n",
    "    bn_layer = nn.BatchNorm1d(num_features, affine=False)  # No learnable params for fair comparison\n",
    "    bn_layer.eval()  # Use batch statistics, not running stats\n",
    "    bn_layer.train()  # Actually, use training mode to compute batch stats\n",
    "    \n",
    "    x_pytorch = bn_layer(x)\n",
    "    \n",
    "    print(\"PyTorch BatchNorm1d:\")\n",
    "    print(f\"Mean per feature: {x_pytorch.mean(dim=0)}\")\n",
    "    print(f\"Std per feature: {x_pytorch.std(dim=0, unbiased=False)}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if they're close\n",
    "    print(f\"Manual and PyTorch results are close: {torch.allclose(x_manual, x_pytorch, atol=1e-6)}\")\n",
    "    \n",
    "    return x, x_manual, x_pytorch\n",
    "\n",
    "# Run the test\n",
    "x_orig, x_manual, x_pytorch = test_batch_norm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Batch Norm with Learnable Parameters (Affine Transform)\n",
    "def test_batch_norm_with_params():\n",
    "    \"\"\"Test batch norm with gamma (scale) and beta (shift) parameters\"\"\"\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    batch_size, num_features = 6, 3\n",
    "    x = torch.randn(batch_size, num_features) * 2 + 1\n",
    "    \n",
    "    print(\"Testing Batch Norm with learnable parameters:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Input mean per feature: {x.mean(dim=0)}\")\n",
    "    print(f\"Input std per feature: {x.std(dim=0, unbiased=False)}\")\n",
    "    print()\n",
    "    \n",
    "    # Manual batch norm with learnable parameters\n",
    "    def batch_norm_with_params(x, gamma, beta, eps=1e-5):\n",
    "        # Normalize to mean=0, std=1\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        var = x.var(dim=0, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "        \n",
    "        # Apply learnable affine transformation\n",
    "        x_out = gamma * x_norm + beta\n",
    "        return x_out\n",
    "    \n",
    "    # Define learnable parameters\n",
    "    gamma = torch.tensor([2.0, 0.5, 1.5])  # Scale factors\n",
    "    beta = torch.tensor([1.0, -1.0, 0.0])   # Shift factors\n",
    "    \n",
    "    # Apply manual batch norm\n",
    "    x_manual = batch_norm_with_params(x, gamma, beta)\n",
    "    \n",
    "    print(\"After manual batch norm with learnable params:\")\n",
    "    print(f\"Output mean per feature: {x_manual.mean(dim=0)}\")\n",
    "    print(f\"Output std per feature: {x_manual.std(dim=0, unbiased=False)}\")\n",
    "    print(f\"Expected std (gamma values): {gamma}\")\n",
    "    print(f\"Expected mean (beta values): {beta}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare with PyTorch BatchNorm1d with affine=True\n",
    "    bn_pytorch = nn.BatchNorm1d(num_features, affine=True)\n",
    "    bn_pytorch.train()\n",
    "    \n",
    "    # Set the same gamma and beta values\n",
    "    with torch.no_grad():\n",
    "        bn_pytorch.weight.copy_(gamma)\n",
    "        bn_pytorch.bias.copy_(beta)\n",
    "    \n",
    "    x_pytorch = bn_pytorch(x)\n",
    "    \n",
    "    print(\"PyTorch BatchNorm1d with same parameters:\")\n",
    "    print(f\"Output mean per feature: {x_pytorch.mean(dim=0)}\")\n",
    "    print(f\"Output std per feature: {x_pytorch.std(dim=0, unbiased=False)}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Results match: {torch.allclose(x_manual, x_pytorch, atol=1e-6)}\")\n",
    "    \n",
    "    return x, x_manual, x_pytorch\n",
    "\n",
    "# Run the test\n",
    "test_batch_norm_with_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete BatchNorm class implementation\n",
    "class ManualBatchNorm1d(torch.nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        \n",
    "        # Learnable parameters (if affine=True)\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.ones(num_features))\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.register_parameter('gamma', None)\n",
    "            self.register_parameter('beta', None)\n",
    "            \n",
    "        # Running statistics (for inference)\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x should be of shape (N, C) or (N, C, L)\n",
    "        if self.training:\n",
    "            # Training mode: use batch statistics\n",
    "            if x.dim() == 2:\n",
    "                mean = x.mean(0)\n",
    "                var = x.var(0, unbiased=False)\n",
    "            else:\n",
    "                # For 3D inputs (N, C, L), compute stats over N and L dimensions\n",
    "                mean = x.mean((0, 2))\n",
    "                var = x.var((0, 2), unbiased=False)\n",
    "            \n",
    "            # Update running statistics\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "                self.num_batches_tracked.add_(1)\n",
    "        else:\n",
    "            # Inference mode: use running statistics\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        if self.affine:\n",
    "            if x.dim() == 2:\n",
    "                x_normalized = self.gamma * x_normalized + self.beta\n",
    "            else:\n",
    "                # Reshape for broadcasting with 3D inputs\n",
    "                x_normalized = self.gamma.view(1, -1, 1) * x_normalized + self.beta.view(1, -1, 1)\n",
    "        \n",
    "        return x_normalized\n",
    "\n",
    "# Test our custom BatchNorm\n",
    "print(\"Testing custom BatchNorm class:\")\n",
    "x_test = torch.randn(4, 3, 5)  # (N, C, L)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "\n",
    "# Initialize our custom BatchNorm\n",
    "custom_bn = ManualBatchNorm1d(3)\n",
    "custom_bn.train()\n",
    "\n",
    "# Forward pass\n",
    "output = custom_bn(x_test)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output mean per channel: {output.mean((0, 2))}\")\n",
    "print(f\"Output std per channel: {output.std((0, 2), unbiased=False)}\")\n",
    "\n",
    "# Compare with PyTorch's BatchNorm1d\n",
    "pytorch_bn = torch.nn.BatchNorm1d(3)\n",
    "pytorch_output = pytorch_bn(x_test)\n",
    "print(f\"PyTorch BatchNorm mean per channel: {pytorch_output.mean((0, 2))}\")\n",
    "print(f\"PyTorch BatchNorm std per channel: {pytorch_output.std((0, 2), unbiased=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 10, 2\n",
    "q = torch.randn((B, T, C))\n",
    "k = torch.randn((B, T, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. True Label (What the image *actually* is) ---\n",
    "# In PyTorch for CrossEntropyLoss, the target (true label) should be\n",
    "# an integer representing the class index.\n",
    "# Let's say: 0 = cat, 1 = dog, 2 = bird\n",
    "true_label = torch.tensor([1]) # This image is a DOG (index 1)\n",
    "print(f\"True Label (index): {true_label}\")\n",
    "\n",
    "# --- 2. Model's Predictions (Logits) ---\n",
    "# Our model outputs \"logits\" before the softmax. Logits are raw, unnormalized scores.\n",
    "# Higher scores mean the model is more confident in that class.\n",
    "# Example: We predict it's a cat with score 0.5, dog with 2.0, bird with 0.1\n",
    "# The higher the logit, the more the model \"thinks\" it's that class.\n",
    "model_predictions_logits = torch.tensor([[0.5, 2.0, 0.1]])\n",
    "# The shape is (batch_size, num_classes). Here, batch_size=1.\n",
    "print(f\"Model Predictions (logits): {model_predictions_logits}\")\n",
    "\n",
    "# --- 3. Applying Softmax (Optional for CrossEntropyLoss, but good for intuition) ---\n",
    "# CrossEntropyLoss *internally* applies softmax, so you don't do it manually\n",
    "# before passing to the loss function. But let's do it here to see probabilities.\n",
    "probabilities = F.softmax(model_predictions_logits, dim=1)\n",
    "print(f\"Model Predictions (probabilities after softmax): {probabilities}\")\n",
    "# Notice: The dog probability (0.83) is highest, which is good since it's a dog.\n",
    "\n",
    "# --- 4. Calculate Cross-Entropy Loss ---\n",
    "# PyTorch's `nn.CrossEntropyLoss` is designed to take raw logits and true labels.\n",
    "# It combines LogSoftmax and NLLLoss (Negative Log Likelihood Loss) for numerical stability.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss = loss_function(model_predictions_logits, true_label)\n",
    "\n",
    "print(f\"\\nCalculated Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# --- Let's see what happens if the model was wrong ---\n",
    "print(\"\\n--- Scenario 2: Model is wrong but confident ---\")\n",
    "# Suppose the true label is still DOG (index 1)\n",
    "true_label_wrong_scenario = torch.tensor([1])\n",
    "\n",
    "# But the model confidently predicted it was a CAT (index 0)\n",
    "model_predictions_wrong_logits = torch.tensor([[5.0, 0.1, 0.1]]) # High score for cat\n",
    "print(f\"Model Predictions (wrong logits): {model_predictions_wrong_logits}\")\n",
    "\n",
    "probabilities_wrong = F.softmax(model_predictions_wrong_logits, dim=1)\n",
    "print(f\"Model Predictions (probabilities after softmax, wrong): {probabilities_wrong}\")\n",
    "\n",
    "loss_wrong = loss_function(model_predictions_wrong_logits, true_label_wrong_scenario)\n",
    "print(f\"Calculated Cross-Entropy Loss (model confidently wrong): {loss_wrong.item():.4f}\")\n",
    "\n",
    "# --- Let's see what happens if the model was less confident but still wrong ---\n",
    "print(\"\\n--- Scenario 3: Model is wrong but less confident ---\")\n",
    "# True label is still DOG (index 1)\n",
    "true_label_less_confident_wrong_scenario = torch.tensor([1])\n",
    "\n",
    "# Model predicted cat, but less confidently\n",
    "model_predictions_less_confident_wrong_logits = torch.tensor([[1.0, 0.1, 0.1]]) # Lower score for cat\n",
    "print(f\"Model Predictions (less confident wrong logits): {model_predictions_less_confident_wrong_logits}\")\n",
    "\n",
    "probabilities_less_confident_wrong = F.softmax(model_predictions_less_confident_wrong_logits, dim=1)\n",
    "print(f\"Model Predictions (probabilities after softmax, less confident wrong): {probabilities_less_confident_wrong}\")\n",
    "\n",
    "loss_less_confident_wrong = loss_function(model_predictions_less_confident_wrong_logits, true_label_less_confident_wrong_scenario)\n",
    "print(f\"Calculated Cross-Entropy Loss (model less confidently wrong): {loss_less_confident_wrong.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
